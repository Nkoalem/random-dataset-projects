{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsEAIaJ7Dk6L"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress ConvergenceWarning\n",
        "#warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "# Load the iris dataset\n",
        "data = pd.read_csv(\n",
        "    '/content/sample_data/Iris.csv',           # csv file\n",
        "    sep=';',               # Separator is semicolon (;)\n",
        "    header=None,           # No header row in file-first row is data\n",
        "    encoding='utf-8-sig',  # Handle byte-order mark (BOM) at file start\n",
        "    names=[                # Assign column names manually\n",
        "        'sepal_length',\n",
        "        'sepal_width',\n",
        "        'petal_length',\n",
        "        'petal_width',\n",
        "        'species'\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "9yVtJdKrD1O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare/Separate features(X) and labels(y)\n",
        "X = data.iloc[:, :4].values # features column: selcts all rows & first 4 columns\n",
        "y = data.iloc[:, 4].values.reshape(-1,1)  # target column: selects all rows, and only 5th column(using index numbering)"
      ],
      "metadata": {
        "id": "UbudIy-YEicW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2a.normalising the input values using max-min normalisation;\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "mMDe_AfWEtWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2b: one-hot encoding of target values\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_onehot = encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "zigFVHAqHc1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "tfhPnyvAJHGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Split data (80% train, 10% val, 10% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_onehot, test_size=0.2,random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "nvA-lpxFE1uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build/train/evaluate model : Use the sum-of-squares loss\n",
        "model = MLPClassifier(\n",
        "    hidden_layer_sizes=(20,),  # Single hidden layer with 20 nodes\n",
        "    activation='relu',          # Rectified Linear Unit activation - default\n",
        "    solver='adam',              # Optimizer algorithm. Gradient descent to minimise loss\n",
        "    learning_rate_init=0.01,\n",
        "    max_iter=1000,                 # trains 1000 epoch at a time\n",
        "    warm_start=False,            # Continue training across epochs\n",
        "    verbose=True,\n",
        "    random_state=42             # Reproducible results\n",
        ")"
      ],
      "metadata": {
        "id": "3DNNPbvKHlzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracking losses: losses will be appended here\n",
        "train_losses = []\n",
        "val_losses = []\n"
      ],
      "metadata": {
        "id": "dp_Z6WSjHw1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to float for subtraction operations\n",
        "y_train = y_train.astype(float)\n",
        "y_val = y_val.astype(float)"
      ],
      "metadata": {
        "id": "_E4e8eg7H6cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training loop-by-loop (epoch-by-epoch)\n",
        "for epoch in range(100):  # Max 150 epochs\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities (for regression-style loss)\n",
        "  y_train_pred = model.predict_proba(X_train)\n",
        "  y_val_pred = model.predict_proba(X_val)\n",
        "\n",
        "# Sum-of-squares loss: 0.5 * mean squared error across all outputs\n",
        "  train_loss = 0.5 * np.mean(np.sum((y_train - y_train_pred) ** 2, axis=1))\n",
        "  val_loss = 0.5 * np.mean(np.sum((y_val - y_val_pred) ** 2, axis=1))\n",
        "\n",
        "# append to the empty lists\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "    # Print current losses\n",
        "  print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.3f}, Val Loss = {val_loss:.3f}\") # rounded off to 3 decimals\n"
      ],
      "metadata": {
        "id": "OPg0bGPLIDaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46727f5f-990c-4b09-b80d-7125a92f69be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 91: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 92: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 93: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 94: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 95: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 96: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 97: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 98: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 99: Train Loss = 0.016, Val Loss = 0.007\n",
            "Iteration 1, loss = 2.21890430\n",
            "Iteration 2, loss = 2.17481695\n",
            "Iteration 3, loss = 2.13485788\n",
            "Iteration 4, loss = 2.09766822\n",
            "Iteration 5, loss = 2.06334397\n",
            "Iteration 6, loss = 2.03110282\n",
            "Iteration 7, loss = 2.00079616\n",
            "Iteration 8, loss = 1.97217178\n",
            "Iteration 9, loss = 1.94493629\n",
            "Iteration 10, loss = 1.91881886\n",
            "Iteration 11, loss = 1.89373718\n",
            "Iteration 12, loss = 1.86951420\n",
            "Iteration 13, loss = 1.84633106\n",
            "Iteration 14, loss = 1.82411418\n",
            "Iteration 15, loss = 1.80279603\n",
            "Iteration 16, loss = 1.78217133\n",
            "Iteration 17, loss = 1.76205165\n",
            "Iteration 18, loss = 1.74240554\n",
            "Iteration 19, loss = 1.72290593\n",
            "Iteration 20, loss = 1.70330730\n",
            "Iteration 21, loss = 1.68354433\n",
            "Iteration 22, loss = 1.66362568\n",
            "Iteration 23, loss = 1.64339636\n",
            "Iteration 24, loss = 1.62274213\n",
            "Iteration 25, loss = 1.60170161\n",
            "Iteration 26, loss = 1.58020941\n",
            "Iteration 27, loss = 1.55828015\n",
            "Iteration 28, loss = 1.53593123\n",
            "Iteration 29, loss = 1.51314992\n",
            "Iteration 30, loss = 1.48996335\n",
            "Iteration 31, loss = 1.46644065\n",
            "Iteration 32, loss = 1.44264048\n",
            "Iteration 33, loss = 1.41862930\n",
            "Iteration 34, loss = 1.39446174\n",
            "Iteration 35, loss = 1.37027180\n",
            "Iteration 36, loss = 1.34612398\n",
            "Iteration 37, loss = 1.32208358\n",
            "Iteration 38, loss = 1.29828136\n",
            "Iteration 39, loss = 1.27477034\n",
            "Iteration 40, loss = 1.25170830\n",
            "Iteration 41, loss = 1.22908317\n",
            "Iteration 42, loss = 1.20676294\n",
            "Iteration 43, loss = 1.18474020\n",
            "Iteration 44, loss = 1.16307228\n",
            "Iteration 45, loss = 1.14182190\n",
            "Iteration 46, loss = 1.12104392\n",
            "Iteration 47, loss = 1.10081156\n",
            "Iteration 48, loss = 1.08116582\n",
            "Iteration 49, loss = 1.06208708\n",
            "Iteration 50, loss = 1.04358304\n",
            "Iteration 51, loss = 1.02564611\n",
            "Iteration 52, loss = 1.00827160\n",
            "Iteration 53, loss = 0.99145387\n",
            "Iteration 54, loss = 0.97520775\n",
            "Iteration 55, loss = 0.95951371\n",
            "Iteration 56, loss = 0.94434577\n",
            "Iteration 57, loss = 0.92970015\n",
            "Iteration 58, loss = 0.91554969\n",
            "Iteration 59, loss = 0.90187595\n",
            "Iteration 60, loss = 0.88866302\n",
            "Iteration 61, loss = 0.87588317\n",
            "Iteration 62, loss = 0.86351467\n",
            "Iteration 63, loss = 0.85154751\n",
            "Iteration 64, loss = 0.83995370\n",
            "Iteration 65, loss = 0.82871050\n",
            "Iteration 66, loss = 0.81780339\n",
            "Iteration 67, loss = 0.80720776\n",
            "Iteration 68, loss = 0.79691709\n",
            "Iteration 69, loss = 0.78691376\n",
            "Iteration 70, loss = 0.77718266\n",
            "Iteration 71, loss = 0.76773264\n",
            "Iteration 72, loss = 0.75854025\n",
            "Iteration 73, loss = 0.74958706\n",
            "Iteration 74, loss = 0.74084325\n",
            "Iteration 75, loss = 0.73229911\n",
            "Iteration 76, loss = 0.72395160\n",
            "Iteration 77, loss = 0.71580142\n",
            "Iteration 78, loss = 0.70782632\n",
            "Iteration 79, loss = 0.70001716\n",
            "Iteration 80, loss = 0.69236679\n",
            "Iteration 81, loss = 0.68487819\n",
            "Iteration 82, loss = 0.67753558\n",
            "Iteration 83, loss = 0.67033582\n",
            "Iteration 84, loss = 0.66328574\n",
            "Iteration 85, loss = 0.65636616\n",
            "Iteration 86, loss = 0.64957359\n",
            "Iteration 87, loss = 0.64290451\n",
            "Iteration 88, loss = 0.63635356\n",
            "Iteration 89, loss = 0.62991793\n",
            "Iteration 90, loss = 0.62359518\n",
            "Iteration 91, loss = 0.61737849\n",
            "Iteration 92, loss = 0.61126125\n",
            "Iteration 93, loss = 0.60524303\n",
            "Iteration 94, loss = 0.59931851\n",
            "Iteration 95, loss = 0.59348333\n",
            "Iteration 96, loss = 0.58777309\n",
            "Iteration 97, loss = 0.58216781\n",
            "Iteration 98, loss = 0.57666325\n",
            "Iteration 99, loss = 0.57123500\n",
            "Iteration 100, loss = 0.56588318\n",
            "Iteration 101, loss = 0.56061962\n",
            "Iteration 102, loss = 0.55544318\n",
            "Iteration 103, loss = 0.55034030\n",
            "Iteration 104, loss = 0.54530910\n",
            "Iteration 105, loss = 0.54035258\n",
            "Iteration 106, loss = 0.53546741\n",
            "Iteration 107, loss = 0.53064537\n",
            "Iteration 108, loss = 0.52588423\n",
            "Iteration 109, loss = 0.52118655\n",
            "Iteration 110, loss = 0.51655369\n",
            "Iteration 111, loss = 0.51198473\n",
            "Iteration 112, loss = 0.50747226\n",
            "Iteration 113, loss = 0.50301499\n",
            "Iteration 114, loss = 0.49861310\n",
            "Iteration 115, loss = 0.49426640\n",
            "Iteration 116, loss = 0.48997548\n",
            "Iteration 117, loss = 0.48573651\n",
            "Iteration 118, loss = 0.48154635\n",
            "Iteration 119, loss = 0.47740593\n",
            "Iteration 120, loss = 0.47331539\n",
            "Iteration 121, loss = 0.46927376\n",
            "Iteration 122, loss = 0.46527939\n",
            "Iteration 123, loss = 0.46133016\n",
            "Iteration 124, loss = 0.45742732\n",
            "Iteration 125, loss = 0.45356900\n",
            "Iteration 126, loss = 0.44975222\n",
            "Iteration 127, loss = 0.44596378\n",
            "Iteration 128, loss = 0.44220557\n",
            "Iteration 129, loss = 0.43847395\n",
            "Iteration 130, loss = 0.43478174\n",
            "Iteration 131, loss = 0.43113457\n",
            "Iteration 132, loss = 0.42752674\n",
            "Iteration 133, loss = 0.42395492\n",
            "Iteration 134, loss = 0.42041852\n",
            "Iteration 135, loss = 0.41692258\n",
            "Iteration 136, loss = 0.41346346\n",
            "Iteration 137, loss = 0.41004510\n",
            "Iteration 138, loss = 0.40666970\n",
            "Iteration 139, loss = 0.40333403\n",
            "Iteration 140, loss = 0.40004788\n",
            "Iteration 141, loss = 0.39679666\n",
            "Iteration 142, loss = 0.39357492\n",
            "Iteration 143, loss = 0.39038290\n",
            "Iteration 144, loss = 0.38722117\n",
            "Iteration 145, loss = 0.38408940\n",
            "Iteration 146, loss = 0.38097924\n",
            "Iteration 147, loss = 0.37789884\n",
            "Iteration 148, loss = 0.37484732\n",
            "Iteration 149, loss = 0.37182443\n",
            "Iteration 150, loss = 0.36882953\n",
            "Iteration 151, loss = 0.36586216\n",
            "Iteration 152, loss = 0.36292782\n",
            "Iteration 153, loss = 0.36002064\n",
            "Iteration 154, loss = 0.35712810\n",
            "Iteration 155, loss = 0.35425729\n",
            "Iteration 156, loss = 0.35140763\n",
            "Iteration 157, loss = 0.34857882\n",
            "Iteration 158, loss = 0.34577103\n",
            "Iteration 159, loss = 0.34298443\n",
            "Iteration 160, loss = 0.34021957\n",
            "Iteration 161, loss = 0.33747639\n",
            "Iteration 162, loss = 0.33473610\n",
            "Iteration 163, loss = 0.33200305\n",
            "Iteration 164, loss = 0.32927815\n",
            "Iteration 165, loss = 0.32657127\n",
            "Iteration 166, loss = 0.32388107\n",
            "Iteration 167, loss = 0.32120874\n",
            "Iteration 168, loss = 0.31855966\n",
            "Iteration 169, loss = 0.31594613\n",
            "Iteration 170, loss = 0.31337302\n",
            "Iteration 171, loss = 0.31084134\n",
            "Iteration 172, loss = 0.30836938\n",
            "Iteration 173, loss = 0.30595283\n",
            "Iteration 174, loss = 0.30357773\n",
            "Iteration 175, loss = 0.30123666\n",
            "Iteration 176, loss = 0.29892868\n",
            "Iteration 177, loss = 0.29665078\n",
            "Iteration 178, loss = 0.29440594\n",
            "Iteration 179, loss = 0.29219652\n",
            "Iteration 180, loss = 0.29002119\n",
            "Iteration 181, loss = 0.28787530\n",
            "Iteration 182, loss = 0.28575917\n",
            "Iteration 183, loss = 0.28367415\n",
            "Iteration 184, loss = 0.28161838\n",
            "Iteration 185, loss = 0.27959068\n",
            "Iteration 186, loss = 0.27759319\n",
            "Iteration 187, loss = 0.27563123\n",
            "Iteration 188, loss = 0.27369764\n",
            "Iteration 189, loss = 0.27179011\n",
            "Iteration 190, loss = 0.26991133\n",
            "Iteration 191, loss = 0.26805791\n",
            "Iteration 192, loss = 0.26622895\n",
            "Iteration 193, loss = 0.26442548\n",
            "Iteration 194, loss = 0.26265056\n",
            "Iteration 195, loss = 0.26089982\n",
            "Iteration 196, loss = 0.25917491\n",
            "Iteration 197, loss = 0.25747582\n",
            "Iteration 198, loss = 0.25579824\n",
            "Iteration 199, loss = 0.25414439\n",
            "Iteration 200, loss = 0.25251558\n",
            "Iteration 201, loss = 0.25091096\n",
            "Iteration 202, loss = 0.24933051\n",
            "Iteration 203, loss = 0.24777296\n",
            "Iteration 204, loss = 0.24623698\n",
            "Iteration 205, loss = 0.24472214\n",
            "Iteration 206, loss = 0.24323389\n",
            "Iteration 207, loss = 0.24176367\n",
            "Iteration 208, loss = 0.24031248\n",
            "Iteration 209, loss = 0.23888308\n",
            "Iteration 210, loss = 0.23747496\n",
            "Iteration 211, loss = 0.23608402\n",
            "Iteration 212, loss = 0.23471813\n",
            "Iteration 213, loss = 0.23337002\n",
            "Iteration 214, loss = 0.23202549\n",
            "Iteration 215, loss = 0.23069670\n",
            "Iteration 216, loss = 0.22938571\n",
            "Iteration 217, loss = 0.22809493\n",
            "Iteration 218, loss = 0.22681971\n",
            "Iteration 219, loss = 0.22556309\n",
            "Iteration 220, loss = 0.22432557\n",
            "Iteration 221, loss = 0.22310541\n",
            "Iteration 222, loss = 0.22190176\n",
            "Iteration 223, loss = 0.22071863\n",
            "Iteration 224, loss = 0.21955149\n",
            "Iteration 225, loss = 0.21840137\n",
            "Iteration 226, loss = 0.21726688\n",
            "Iteration 227, loss = 0.21614784\n",
            "Iteration 228, loss = 0.21504577\n",
            "Iteration 229, loss = 0.21396056\n",
            "Iteration 230, loss = 0.21286913\n",
            "Iteration 231, loss = 0.21178367\n",
            "Iteration 232, loss = 0.21071198\n",
            "Iteration 233, loss = 0.20965482\n",
            "Iteration 234, loss = 0.20861069\n",
            "Iteration 235, loss = 0.20757866\n",
            "Iteration 236, loss = 0.20656033\n",
            "Iteration 237, loss = 0.20555461\n",
            "Iteration 238, loss = 0.20456427\n",
            "Iteration 239, loss = 0.20361188\n",
            "Iteration 240, loss = 0.20267575\n",
            "Iteration 241, loss = 0.20173113\n",
            "Iteration 242, loss = 0.20079803\n",
            "Iteration 243, loss = 0.19987726\n",
            "Iteration 244, loss = 0.19896882\n",
            "Iteration 245, loss = 0.19807216\n",
            "Iteration 246, loss = 0.19718691\n",
            "Iteration 247, loss = 0.19631293\n",
            "Iteration 248, loss = 0.19545043\n",
            "Iteration 249, loss = 0.19459776\n",
            "Iteration 250, loss = 0.19375498\n",
            "Iteration 251, loss = 0.19292221\n",
            "Iteration 252, loss = 0.19209950\n",
            "Iteration 253, loss = 0.19128703\n",
            "Iteration 254, loss = 0.19048419\n",
            "Iteration 255, loss = 0.18969637\n",
            "Iteration 256, loss = 0.18892016\n",
            "Iteration 257, loss = 0.18815243\n",
            "Iteration 258, loss = 0.18739307\n",
            "Iteration 259, loss = 0.18664246\n",
            "Iteration 260, loss = 0.18589984\n",
            "Iteration 261, loss = 0.18516581\n",
            "Iteration 262, loss = 0.18443991\n",
            "Iteration 263, loss = 0.18372236\n",
            "Iteration 264, loss = 0.18301266\n",
            "Iteration 265, loss = 0.18231113\n",
            "Iteration 266, loss = 0.18161736\n",
            "Iteration 267, loss = 0.18093128\n",
            "Iteration 268, loss = 0.18025292\n",
            "Iteration 269, loss = 0.17958227\n",
            "Iteration 270, loss = 0.17891908\n",
            "Iteration 271, loss = 0.17826324\n",
            "Iteration 272, loss = 0.17761473\n",
            "Iteration 273, loss = 0.17697406\n",
            "Iteration 274, loss = 0.17633993\n",
            "Iteration 275, loss = 0.17571328\n",
            "Iteration 276, loss = 0.17509322\n",
            "Iteration 277, loss = 0.17447997\n",
            "Iteration 278, loss = 0.17387374\n",
            "Iteration 279, loss = 0.17327501\n",
            "Iteration 280, loss = 0.17268114\n",
            "Iteration 281, loss = 0.17209464\n",
            "Iteration 282, loss = 0.17151458\n",
            "Iteration 283, loss = 0.17094069\n",
            "Iteration 284, loss = 0.17037280\n",
            "Iteration 285, loss = 0.16981099\n",
            "Iteration 286, loss = 0.16925532\n",
            "Iteration 287, loss = 0.16870558\n",
            "Iteration 288, loss = 0.16816155\n",
            "Iteration 289, loss = 0.16762319\n",
            "Iteration 290, loss = 0.16709056\n",
            "Iteration 291, loss = 0.16656360\n",
            "Iteration 292, loss = 0.16604215\n",
            "Iteration 293, loss = 0.16552618\n",
            "Iteration 294, loss = 0.16501535\n",
            "Iteration 295, loss = 0.16450959\n",
            "Iteration 296, loss = 0.16400910\n",
            "Iteration 297, loss = 0.16351394\n",
            "Iteration 298, loss = 0.16302367\n",
            "Iteration 299, loss = 0.16253816\n",
            "Iteration 300, loss = 0.16205755\n",
            "Iteration 301, loss = 0.16158186\n",
            "Iteration 302, loss = 0.16111098\n",
            "Iteration 303, loss = 0.16064472\n",
            "Iteration 304, loss = 0.16018301\n",
            "Iteration 305, loss = 0.15972600\n",
            "Iteration 306, loss = 0.15927350\n",
            "Iteration 307, loss = 0.15882539\n",
            "Iteration 308, loss = 0.15838166\n",
            "Iteration 309, loss = 0.15794223\n",
            "Iteration 310, loss = 0.15750711\n",
            "Iteration 311, loss = 0.15707624\n",
            "Iteration 312, loss = 0.15664952\n",
            "Iteration 313, loss = 0.15622685\n",
            "Iteration 314, loss = 0.15580828\n",
            "Iteration 315, loss = 0.15539375\n",
            "Iteration 316, loss = 0.15498317\n",
            "Iteration 317, loss = 0.15457647\n",
            "Iteration 318, loss = 0.15417359\n",
            "Iteration 319, loss = 0.15377453\n",
            "Iteration 320, loss = 0.15337925\n",
            "Iteration 321, loss = 0.15298765\n",
            "Iteration 322, loss = 0.15259973\n",
            "Iteration 323, loss = 0.15221544\n",
            "Iteration 324, loss = 0.15183471\n",
            "Iteration 325, loss = 0.15145750\n",
            "Iteration 326, loss = 0.15108375\n",
            "Iteration 327, loss = 0.15071343\n",
            "Iteration 328, loss = 0.15034650\n",
            "Iteration 329, loss = 0.14998303\n",
            "Iteration 330, loss = 0.14962279\n",
            "Iteration 331, loss = 0.14926575\n",
            "Iteration 332, loss = 0.14891205\n",
            "Iteration 333, loss = 0.14856155\n",
            "Iteration 334, loss = 0.14821419\n",
            "Iteration 335, loss = 0.14786992\n",
            "Iteration 336, loss = 0.14752872\n",
            "Iteration 337, loss = 0.14719056\n",
            "Iteration 338, loss = 0.14685584\n",
            "Iteration 339, loss = 0.14652413\n",
            "Iteration 340, loss = 0.14619527\n",
            "Iteration 341, loss = 0.14586937\n",
            "Iteration 342, loss = 0.14554642\n",
            "Iteration 343, loss = 0.14522615\n",
            "Iteration 344, loss = 0.14490855\n",
            "Iteration 345, loss = 0.14459376\n",
            "Iteration 346, loss = 0.14428213\n",
            "Iteration 347, loss = 0.14397335\n",
            "Iteration 348, loss = 0.14366715\n",
            "Iteration 349, loss = 0.14336348\n",
            "Iteration 350, loss = 0.14306235\n",
            "Iteration 351, loss = 0.14276380\n",
            "Iteration 352, loss = 0.14246803\n",
            "Iteration 353, loss = 0.14217487\n",
            "Iteration 354, loss = 0.14188419\n",
            "Iteration 355, loss = 0.14159583\n",
            "Iteration 356, loss = 0.14130996\n",
            "Iteration 357, loss = 0.14102649\n",
            "Iteration 358, loss = 0.14074545\n",
            "Iteration 359, loss = 0.14046679\n",
            "Iteration 360, loss = 0.14019042\n",
            "Iteration 361, loss = 0.13991642\n",
            "Iteration 362, loss = 0.13964474\n",
            "Iteration 363, loss = 0.13937533\n",
            "Iteration 364, loss = 0.13910818\n",
            "Iteration 365, loss = 0.13884319\n",
            "Iteration 366, loss = 0.13858036\n",
            "Iteration 367, loss = 0.13831964\n",
            "Iteration 368, loss = 0.13806116\n",
            "Iteration 369, loss = 0.13780482\n",
            "Iteration 370, loss = 0.13755045\n",
            "Iteration 371, loss = 0.13729829\n",
            "Iteration 372, loss = 0.13704825\n",
            "Iteration 373, loss = 0.13680020\n",
            "Iteration 374, loss = 0.13655411\n",
            "Iteration 375, loss = 0.13630999\n",
            "Iteration 376, loss = 0.13606799\n",
            "Iteration 377, loss = 0.13582760\n",
            "Iteration 378, loss = 0.13558937\n",
            "Iteration 379, loss = 0.13535300\n",
            "Iteration 380, loss = 0.13511845\n",
            "Iteration 381, loss = 0.13488580\n",
            "Iteration 382, loss = 0.13465492\n",
            "Iteration 383, loss = 0.13442589\n",
            "Iteration 384, loss = 0.13419863\n",
            "Iteration 385, loss = 0.13397327\n",
            "Iteration 386, loss = 0.13374957\n",
            "Iteration 387, loss = 0.13352751\n",
            "Iteration 388, loss = 0.13330725\n",
            "Iteration 389, loss = 0.13308866\n",
            "Iteration 390, loss = 0.13287182\n",
            "Iteration 391, loss = 0.13265651\n",
            "Iteration 392, loss = 0.13244304\n",
            "Iteration 393, loss = 0.13223102\n",
            "Iteration 394, loss = 0.13202060\n",
            "Iteration 395, loss = 0.13181206\n",
            "Iteration 396, loss = 0.13160471\n",
            "Iteration 397, loss = 0.13139919\n",
            "Iteration 398, loss = 0.13119520\n",
            "Iteration 399, loss = 0.13099262\n",
            "Iteration 400, loss = 0.13079152\n",
            "Iteration 401, loss = 0.13059194\n",
            "Iteration 402, loss = 0.13039392\n",
            "Iteration 403, loss = 0.13019732\n",
            "Iteration 404, loss = 0.13000212\n",
            "Iteration 405, loss = 0.12980837\n",
            "Iteration 406, loss = 0.12961607\n",
            "Iteration 407, loss = 0.12942515\n",
            "Iteration 408, loss = 0.12923568\n",
            "Iteration 409, loss = 0.12904773\n",
            "Iteration 410, loss = 0.12886080\n",
            "Iteration 411, loss = 0.12867546\n",
            "Iteration 412, loss = 0.12849147\n",
            "Iteration 413, loss = 0.12830874\n",
            "Iteration 414, loss = 0.12812731\n",
            "Iteration 415, loss = 0.12794727\n",
            "Iteration 416, loss = 0.12776844\n",
            "Iteration 417, loss = 0.12759089\n",
            "Iteration 418, loss = 0.12741460\n",
            "Iteration 419, loss = 0.12723956\n",
            "Iteration 420, loss = 0.12706576\n",
            "Iteration 421, loss = 0.12689314\n",
            "Iteration 422, loss = 0.12672171\n",
            "Iteration 423, loss = 0.12655157\n",
            "Iteration 424, loss = 0.12638260\n",
            "Iteration 425, loss = 0.12621482\n",
            "Iteration 426, loss = 0.12604824\n",
            "Iteration 427, loss = 0.12588278\n",
            "Iteration 428, loss = 0.12571847\n",
            "Iteration 429, loss = 0.12555527\n",
            "Iteration 430, loss = 0.12539326\n",
            "Iteration 431, loss = 0.12523229\n",
            "Iteration 432, loss = 0.12507257\n",
            "Iteration 433, loss = 0.12491391\n",
            "Iteration 434, loss = 0.12475631\n",
            "Iteration 435, loss = 0.12459977\n",
            "Iteration 436, loss = 0.12444425\n",
            "Iteration 437, loss = 0.12428975\n",
            "Iteration 438, loss = 0.12413641\n",
            "Iteration 439, loss = 0.12398406\n",
            "Iteration 440, loss = 0.12383268\n",
            "Iteration 441, loss = 0.12368241\n",
            "Iteration 442, loss = 0.12353315\n",
            "Iteration 443, loss = 0.12338487\n",
            "Iteration 444, loss = 0.12323755\n",
            "Iteration 445, loss = 0.12309138\n",
            "Iteration 446, loss = 0.12294602\n",
            "Iteration 447, loss = 0.12280157\n",
            "Iteration 448, loss = 0.12265821\n",
            "Iteration 449, loss = 0.12251575\n",
            "Iteration 450, loss = 0.12237420\n",
            "Iteration 451, loss = 0.12223356\n",
            "Iteration 452, loss = 0.12209392\n",
            "Iteration 453, loss = 0.12195512\n",
            "Iteration 454, loss = 0.12181730\n",
            "Iteration 455, loss = 0.12168039\n",
            "Iteration 456, loss = 0.12154433\n",
            "Iteration 457, loss = 0.12140912\n",
            "Iteration 458, loss = 0.12127475\n",
            "Iteration 459, loss = 0.12114122\n",
            "Iteration 460, loss = 0.12100880\n",
            "Iteration 461, loss = 0.12087707\n",
            "Iteration 462, loss = 0.12074598\n",
            "Iteration 463, loss = 0.12061605\n",
            "Iteration 464, loss = 0.12048692\n",
            "Iteration 465, loss = 0.12035857\n",
            "Iteration 466, loss = 0.12023081\n",
            "Iteration 467, loss = 0.12010292\n",
            "Iteration 468, loss = 0.11997508\n",
            "Iteration 469, loss = 0.11984737\n",
            "Iteration 470, loss = 0.11971987\n",
            "Iteration 471, loss = 0.11959273\n",
            "Iteration 472, loss = 0.11946617\n",
            "Iteration 473, loss = 0.11934014\n",
            "Iteration 474, loss = 0.11921476\n",
            "Iteration 475, loss = 0.11908992\n",
            "Iteration 476, loss = 0.11896575\n",
            "Iteration 477, loss = 0.11884210\n",
            "Iteration 478, loss = 0.11871916\n",
            "Iteration 479, loss = 0.11859688\n",
            "Iteration 480, loss = 0.11847531\n",
            "Iteration 481, loss = 0.11835461\n",
            "Iteration 482, loss = 0.11823466\n",
            "Iteration 483, loss = 0.11811544\n",
            "Iteration 484, loss = 0.11799704\n",
            "Iteration 485, loss = 0.11787945\n",
            "Iteration 486, loss = 0.11776259\n",
            "Iteration 487, loss = 0.11764658\n",
            "Iteration 488, loss = 0.11753174\n",
            "Iteration 489, loss = 0.11741764\n",
            "Iteration 490, loss = 0.11730435\n",
            "Iteration 491, loss = 0.11719178\n",
            "Iteration 492, loss = 0.11708005\n",
            "Iteration 493, loss = 0.11696911\n",
            "Iteration 494, loss = 0.11685882\n",
            "Iteration 495, loss = 0.11674934\n",
            "Iteration 496, loss = 0.11664063\n",
            "Iteration 497, loss = 0.11653302\n",
            "Iteration 498, loss = 0.11642596\n",
            "Iteration 499, loss = 0.11631980\n",
            "Iteration 500, loss = 0.11621432\n",
            "Iteration 501, loss = 0.11610947\n",
            "Iteration 502, loss = 0.11600524\n",
            "Iteration 503, loss = 0.11590167\n",
            "Iteration 504, loss = 0.11579875\n",
            "Iteration 505, loss = 0.11569686\n",
            "Iteration 506, loss = 0.11559558\n",
            "Iteration 507, loss = 0.11549475\n",
            "Iteration 508, loss = 0.11539475\n",
            "Iteration 509, loss = 0.11529532\n",
            "Iteration 510, loss = 0.11519646\n",
            "Iteration 511, loss = 0.11509847\n",
            "Iteration 512, loss = 0.11500081\n",
            "Iteration 513, loss = 0.11490374\n",
            "Iteration 514, loss = 0.11480741\n",
            "Iteration 515, loss = 0.11471188\n",
            "Iteration 516, loss = 0.11461690\n",
            "Iteration 517, loss = 0.11452244\n",
            "Iteration 518, loss = 0.11442869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Epoch 100: Train Loss = 0.016, Val Loss = 0.007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Evaluate on test set\n",
        "# Predict class labels on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVvyL0HNaPtf",
        "outputId": "5df12fec-7141-4610-b3f2-714db97010de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.933\n"
          ]
        }
      ]
    }
  ]
}